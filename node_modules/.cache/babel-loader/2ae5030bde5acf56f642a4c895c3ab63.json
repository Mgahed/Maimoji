{"ast":null,"code":"import * as faceapi from 'face-api.js'; // Load models and weights\n\nexport async function loadModels() {\n  const MODEL_URL = process.env.PUBLIC_URL + '/models';\n  await faceapi.loadTinyFaceDetectorModel(MODEL_URL);\n  await faceapi.loadFaceLandmarkTinyModel(MODEL_URL);\n  await faceapi.loadFaceRecognitionModel(MODEL_URL);\n  await faceapi.loadFaceExpressionModel(MODEL_URL); // alert(MODEL_URL)\n}\nexport async function getFullFaceDescription(blob, inputSize = 512) {\n  // const detections = await faceapi.detectAllFaces(blob, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n  // const resizedDetections = faceapi.resizeResults(detections, inputSize);\n  // var neutral = detections[0].expressions.neutral;\n  // console.log(neutral)\n  // var happy = detections[0].expressions.happy;\n  // var sad = detections[0].expressions.sad;\n  // tiny_face_detector options;\n  let scoreThreshold = 0.5;\n  const OPTION = new faceapi.TinyFaceDetectorOptions({\n    inputSize,\n    scoreThreshold\n  }); // const useTinyModel = true;\n  // fetch image to api\n\n  let img = await faceapi.fetchImage(blob); // detect all faces and generate full description from image\n  // including landmark and descriptor of each face\n\n  let fullDesc = await faceapi.detectAllFaces(img, OPTION).withFaceExpressions();\n  console.log(fullDesc);\n  var neutral = \"\";\n  var happy = \"\";\n  var sad = \"\";\n\n  try {\n    neutral = fullDesc[0].expressions.neutral;\n    happy = fullDesc[0].expressions.happy;\n    sad = fullDesc[0].expressions.sad;\n    sessionStorage.setItem(\"neutral\");\n    console.log(\"neutarl = \" + neutral);\n    console.log(\"happy = \" + happy);\n    console.log(\"sad = \" + sad);\n  } catch {\n    console.log(\"No Dedection\");\n  }\n\n  return fullDesc;\n}\nconst maxDescriptorDistance = 0.5;\nexport async function createMatcher(faceProfile) {\n  // Create labeled descriptors of member from profile\n  let members = Object.keys(faceProfile);\n  let labeledDescriptors = members.map(member => new faceapi.LabeledFaceDescriptors(faceProfile[member].name, faceProfile[member].descriptors.map(descriptor => new Float32Array(descriptor)))); // Create face matcher (maximum descriptor distance is 0.5)\n\n  let faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, maxDescriptorDistance);\n  return faceMatcher;\n}","map":{"version":3,"sources":["C:/Users/Lenovo/maimoji/src/api/face.js"],"names":["faceapi","loadModels","MODEL_URL","process","env","PUBLIC_URL","loadTinyFaceDetectorModel","loadFaceLandmarkTinyModel","loadFaceRecognitionModel","loadFaceExpressionModel","getFullFaceDescription","blob","inputSize","scoreThreshold","OPTION","TinyFaceDetectorOptions","img","fetchImage","fullDesc","detectAllFaces","withFaceExpressions","console","log","neutral","happy","sad","expressions","sessionStorage","setItem","maxDescriptorDistance","createMatcher","faceProfile","members","Object","keys","labeledDescriptors","map","member","LabeledFaceDescriptors","name","descriptors","descriptor","Float32Array","faceMatcher","FaceMatcher"],"mappings":"AAAA,OAAO,KAAKA,OAAZ,MAAyB,aAAzB,C,CAEA;;AACA,OAAO,eAAeC,UAAf,GAA4B;AACjC,QAAMC,SAAS,GAAGC,OAAO,CAACC,GAAR,CAAYC,UAAZ,GAAyB,SAA3C;AACA,QAAML,OAAO,CAACM,yBAAR,CAAkCJ,SAAlC,CAAN;AACA,QAAMF,OAAO,CAACO,yBAAR,CAAkCL,SAAlC,CAAN;AACA,QAAMF,OAAO,CAACQ,wBAAR,CAAiCN,SAAjC,CAAN;AACA,QAAMF,OAAO,CAACS,uBAAR,CAAgCP,SAAhC,CAAN,CALiC,CAMjC;AACD;AAED,OAAO,eAAeQ,sBAAf,CAAsCC,IAAtC,EAA4CC,SAAS,GAAG,GAAxD,EAA6D;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAIC,cAAc,GAAG,GAArB;AACA,QAAMC,MAAM,GAAG,IAAId,OAAO,CAACe,uBAAZ,CAAoC;AACjDH,IAAAA,SADiD;AAEjDC,IAAAA;AAFiD,GAApC,CAAf,CATkE,CAalE;AAEA;;AACA,MAAIG,GAAG,GAAG,MAAMhB,OAAO,CAACiB,UAAR,CAAmBN,IAAnB,CAAhB,CAhBkE,CAkBlE;AACA;;AACA,MAAIO,QAAQ,GAAG,MAAMlB,OAAO,CACzBmB,cADkB,CACHH,GADG,EACEF,MADF,EAElBM,mBAFkB,EAArB;AAIAC,EAAAA,OAAO,CAACC,GAAR,CAAYJ,QAAZ;AACA,MAAIK,OAAO,GAAG,EAAd;AACA,MAAIC,KAAK,GAAG,EAAZ;AACA,MAAIC,GAAG,GAAG,EAAV;;AACA,MAAI;AACFF,IAAAA,OAAO,GAAGL,QAAQ,CAAC,CAAD,CAAR,CAAYQ,WAAZ,CAAwBH,OAAlC;AACAC,IAAAA,KAAK,GAAGN,QAAQ,CAAC,CAAD,CAAR,CAAYQ,WAAZ,CAAwBF,KAAhC;AACAC,IAAAA,GAAG,GAAGP,QAAQ,CAAC,CAAD,CAAR,CAAYQ,WAAZ,CAAwBD,GAA9B;AACAE,IAAAA,cAAc,CAACC,OAAf,CAAuB,SAAvB;AACAP,IAAAA,OAAO,CAACC,GAAR,CAAY,eAAeC,OAA3B;AACAF,IAAAA,OAAO,CAACC,GAAR,CAAY,aAAaE,KAAzB;AACAH,IAAAA,OAAO,CAACC,GAAR,CAAY,WAAWG,GAAvB;AACD,GARD,CAQE,MAAK;AAAEJ,IAAAA,OAAO,CAACC,GAAR,CAAY,cAAZ;AAA6B;;AACtC,SAAOJ,QAAP;AACD;AAED,MAAMW,qBAAqB,GAAG,GAA9B;AACA,OAAO,eAAeC,aAAf,CAA6BC,WAA7B,EAA0C;AAC/C;AACA,MAAIC,OAAO,GAAGC,MAAM,CAACC,IAAP,CAAYH,WAAZ,CAAd;AACA,MAAII,kBAAkB,GAAGH,OAAO,CAACI,GAAR,CACvBC,MAAM,IACJ,IAAIrC,OAAO,CAACsC,sBAAZ,CACEP,WAAW,CAACM,MAAD,CAAX,CAAoBE,IADtB,EAEER,WAAW,CAACM,MAAD,CAAX,CAAoBG,WAApB,CAAgCJ,GAAhC,CACEK,UAAU,IAAI,IAAIC,YAAJ,CAAiBD,UAAjB,CADhB,CAFF,CAFqB,CAAzB,CAH+C,CAa/C;;AACA,MAAIE,WAAW,GAAG,IAAI3C,OAAO,CAAC4C,WAAZ,CAChBT,kBADgB,EAEhBN,qBAFgB,CAAlB;AAIA,SAAOc,WAAP;AACD","sourcesContent":["import * as faceapi from 'face-api.js';\r\n\r\n// Load models and weights\r\nexport async function loadModels() {\r\n  const MODEL_URL = process.env.PUBLIC_URL + '/models';\r\n  await faceapi.loadTinyFaceDetectorModel(MODEL_URL);\r\n  await faceapi.loadFaceLandmarkTinyModel(MODEL_URL);\r\n  await faceapi.loadFaceRecognitionModel(MODEL_URL);\r\n  await faceapi.loadFaceExpressionModel(MODEL_URL);\r\n  // alert(MODEL_URL)\r\n}\r\n\r\nexport async function getFullFaceDescription(blob, inputSize = 512) {\r\n  // const detections = await faceapi.detectAllFaces(blob, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\r\n  // const resizedDetections = faceapi.resizeResults(detections, inputSize);\r\n  // var neutral = detections[0].expressions.neutral;\r\n  // console.log(neutral)\r\n  // var happy = detections[0].expressions.happy;\r\n  // var sad = detections[0].expressions.sad;\r\n  // tiny_face_detector options;\r\n  let scoreThreshold = 0.5;\r\n  const OPTION = new faceapi.TinyFaceDetectorOptions({\r\n    inputSize,\r\n    scoreThreshold\r\n  });\r\n  // const useTinyModel = true;\r\n\r\n  // fetch image to api\r\n  let img = await faceapi.fetchImage(blob);\r\n\r\n  // detect all faces and generate full description from image\r\n  // including landmark and descriptor of each face\r\n  let fullDesc = await faceapi\r\n    .detectAllFaces(img, OPTION)\r\n    .withFaceExpressions();\r\n\r\n  console.log(fullDesc)\r\n  var neutral = \"\";\r\n  var happy = \"\";\r\n  var sad = \"\";\r\n  try {\r\n    neutral = fullDesc[0].expressions.neutral;\r\n    happy = fullDesc[0].expressions.happy;\r\n    sad = fullDesc[0].expressions.sad;\r\n    sessionStorage.setItem(\"neutral\")\r\n    console.log(\"neutarl = \" + neutral)\r\n    console.log(\"happy = \" + happy)\r\n    console.log(\"sad = \" + sad)\r\n  } catch{ console.log(\"No Dedection\") }\r\n  return fullDesc;\r\n}\r\n\r\nconst maxDescriptorDistance = 0.5;\r\nexport async function createMatcher(faceProfile) {\r\n  // Create labeled descriptors of member from profile\r\n  let members = Object.keys(faceProfile);\r\n  let labeledDescriptors = members.map(\r\n    member =>\r\n      new faceapi.LabeledFaceDescriptors(\r\n        faceProfile[member].name,\r\n        faceProfile[member].descriptors.map(\r\n          descriptor => new Float32Array(descriptor)\r\n        )\r\n      )\r\n  );\r\n\r\n  // Create face matcher (maximum descriptor distance is 0.5)\r\n  let faceMatcher = new faceapi.FaceMatcher(\r\n    labeledDescriptors,\r\n    maxDescriptorDistance\r\n  );\r\n  return faceMatcher;\r\n}\r\n"]},"metadata":{},"sourceType":"module"}